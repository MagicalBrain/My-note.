# 神经网络

---

# 回归问题
## 激活函数

用来做逻辑回归的，每一层都要

### sigmoid函数

现在不常用了

```matlab
function y = sigmoid(x)
    s = size(x);
    a = ones(s);
    y = a ./ (a  + exp(-x));
```

### ReLU 函数

```matlab
function y = ReLU(x)
    s = size(x);
    y = zeros(s);
    for i = 1:s(1)
        for j = 1:s(2)
            if ( x(i,j) <= 0)
                y(i,j) = 0;
            else
                y(i,j) = x(i,j);
            end
        end
    end
```

### 激活函数的所需要满足的特性

1、非线性: 即导数不是常数,不然就退化成直线。对于一些画一条直线仍然无法分开的问题，非线性可以把直线掰弯，自从变弯以后，就能包罗万象了。

2、几乎处处可导：也就是具备“丝滑的特性”，（其实是平滑）不要应激过度，要做正常人。数学上，处处可导为后面降到的后向传播算法（BP算法）提供了核心条件

3、输出范围有限：一般是限定在[0,1]，有限的输出范围使得神经元对于一些比较大的输入也会比较稳定。

4、非饱和性：饱和就是指，当输入比较大的时候，输出几乎没变化了，那么会导致梯度消失！什么是梯度消失：就是你天天给女生送花，一开始妹纸还惊喜，到后来直接麻木没反应了。梯度消失带来的负面影响就是会限制了神经网络表达能力，词穷的感觉你有过么。sigmod，tanh函数都是软饱和的，阶跃函数是硬饱和。软是指输入趋于无穷大的时候输出无限接近上线，硬是指像阶跃函数那样，输入非0输出就已经始终都是上限值。数学表示我就懒得写了，传送门在此（https://www.cnblogs.com/rgvb178/p/6055213.html）,里面有写到。如果激活函数是饱和的，带来的缺陷就是系统迭代更新变慢，系统收敛就慢，当然这是可以有办法弥补的，一种方法是使用交叉熵函数作为损失函数，这里不多说。ReLU是非饱和的，亲测效果挺不错，所以这货最近挺火的。

5、单调性：即导数符号不变。导出要么一直大于0，要么一直小于0，不要上蹿下跳。导数符号不变，让神经网络训练容易收敛。

## 简单的小例子

**别忘了逻辑回归！！**

```matlab
% 神经网络小算例
x = [0.9 0.1 0.8];
x = x';

a = [0.9 0.3 0.4;
    0.2 0.8 0.2;
    0.1 0.5 0.6];
b = [0.3 0.7 0.5;
    0.6 0.5 0.2;
    0.8 0.1 0.9];


y1 = sigmoid(a*x) ;
y2 = sigmoid(b*y1);
```

# 分类问题
最后输出结果的时候的激活函数是softmax函数

## softmax函数

```matlab
function re = softmax(x)
    re = exp(x) ./ sum( exp(x) );
```

以上均为正向传播

## 误差函数

用于将分类输出的数据与标签相比较的函数，有时候又称之为损失函数

### 交叉熵函数

```matlab
function re = Cross_Entropy_Error(y,t)
    re = t * y;
    re = sum(re);
    re = -re;
```

### 均方误差

```matlab
function re = means(y,t)
    re = y - t;
    re = re.^2;
    re = sum(re);
    re = re / 2;
```

以上均为正向传播
---

接下来是反向传播
## 反向传播
### cross_entropy_error层的反向传播

