# 梯度下降

---

是一种一阶优化方法，因为只需要计算一阶导数

## 批量梯度下降（batch gradient descent）
算法的公式为：
repeat until convergence {
$$\theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1) (for, j = 0 , and j = 1)$$
}

其中：$\alpha$ 是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。


**注意：**
参数更新是要同时更新，正确方法是：

$$ temp0 = \theta_0 - \frac{\partial}{\partial \theta_0} J(\theta_0,\theta_1)$$
$$ temp1 = \theta_1 - \frac{\partial}{\partial \theta_1} J(\theta_0,\theta_1)$$
$$\theta_0 = temp0$$
$$\theta_1 = temp1$$


## 五步
### 1、定义一个机器学习模型

### 2、定义一个损失函数

### 3、用损失函数计算梯度

### 4、用梯度更新权值

### 5、重复前四步直到梯度为0

比一阶优化方法好的是二阶，常用的有牛顿法

## 梯度下降的线性回归

### 线性回归函数

$$h_{\theta}(x) = \theta_0 + \theta_1x$$
$$J(\theta) = \frac{1}{2m} \sum_{i = 1}^m (h_{\theta}(x^i) - y^i)^2$$



